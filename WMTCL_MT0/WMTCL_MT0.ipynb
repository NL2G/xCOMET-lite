{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa11722-12b9-496f-9da6-fae1c4ada4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import faiss\n",
    "\n",
    "import logging\n",
    "from rich.logging import RichHandler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from accelerate import dispatch_model, infer_auto_device_map\n",
    "from accelerate.utils import get_balanced_memory\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from transformers import AutoTokenizer, MT5EncoderModel\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import get_scheduler\n",
    "\n",
    "import gc\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d4da5-82b1-4397-b972-50e9fa726533",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0f411-6d16-4d98-ba76-1f17a0085320",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MKL_SERVICE_FORCE_INTEL'] = '1'\n",
    "SEED = 101\n",
    "MAX_LENGTH = 128\n",
    "DATA_DIR = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd80b79-b82f-4b3a-9029-f9595a185874",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=\"ERROR\",\n",
    "    format=\"%(message)s\",\n",
    "    datefmt=\"[%X]\",\n",
    "    handlers=[RichHandler(rich_tracebacks=True)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7bfb94-e5de-4407-a4fa-8e0e5d5b942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lang_pairs = ['en-de', 'zh-en', 'he-en']\n",
    "lang_pairs = target_lang_pairs + ['de-en', 'en-zh']\n",
    "\n",
    "id2lang = {\n",
    "    'en' : 'english',\n",
    "    'de' : 'german',\n",
    "    'zh' : 'chinese',\n",
    "    'he' : 'hebrew'\n",
    "}\n",
    "\n",
    "score_type2id = {\n",
    "    'da': 1,\n",
    "    'mqm': 2,\n",
    "    'sqm': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282200e-6965-4485-b142-bd28ecd2b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_dsets = load_dataset('nllg/wmt-metrics-data', token='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').filter(lambda x: x['lp'] in lang_pairs)\n",
    "train_dset = wmt_dsets['train'].rename_column('score_type', 'score_type_str')\n",
    "test_dset = wmt_dsets['test'].rename_column('score_type', 'score_type_str').map(lambda x: {'score_type': score_type2id[x['score_type_str']]})\n",
    "\n",
    "train_dset = train_dset.map(lambda x: {'score_type': score_type2id[x['score_type_str']]}).remove_columns(['score_type_str'])\n",
    "test_dset = test_dset.map(lambda x: {'score_type': score_type2id[x['score_type_str']]}).remove_columns(['score_type_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79471b-b273-4f77-86ba-f5f2aa8cf5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bigscience/mt0-large')\n",
    "model = MT5EncoderModel.from_pretrained('bigscience/mt0-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1f43b-0c2d-44e7-b8fd-0be16815a288",
   "metadata": {},
   "source": [
    "## Prepare FAISS for references and translations\n",
    "\n",
    "Note that for normalized vectors: $\\|x-y\\|^2_2=2-2x^Ty \\, \\rightarrow \\, \\min\\left(x^Ty\\right)=\\max\\left((-x)^Ty\\right)=\\min\\left(\\|(-x)-y\\|^2_2\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4741fcb-7965-4e5f-8d21-3490383410c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10b701-125c-4553-b785-ba4401ec3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_faiss(\n",
    "    model,\n",
    "    pool,\n",
    "    sentences,\n",
    "    nlist=100, # voronoi cells for ANN\n",
    "    m=16, # number of centroid IDs in final compressed vectors\n",
    "    bits=8,  # number of bits in each centroid\n",
    "    nprobe=10, # number of cells to search during inference,\n",
    "    save_filepath=None\n",
    "):\n",
    "    embs = model.encode_multi_process(sentences, pool, batch_size=128)\n",
    "    embs = -embs / np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "\n",
    "    d = embs.shape[1]\n",
    "    quantizer = faiss.IndexFlatL2(d)\n",
    "    index = faiss.IndexIVFPQ(quantizer, d, nlist, m, bits)\n",
    "    index.train(embs)\n",
    "    index.add(embs)\n",
    "    index.nprobe = nprobe\n",
    "    if save_filepath is not None:\n",
    "        faiss.write_index(index, save_filepath)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f558e-3286-460a-a259-1fd1dd1f0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp2train_dset = {}\n",
    "lp2ref_index = {}\n",
    "lp2mt_index = {}\n",
    "\n",
    "st = SentenceTransformer('sentence-transformers/sentence-t5-large')\n",
    "pool = st.start_multi_process_pool()\n",
    "for lp in (pbar := tqdm_notebook(lang_pairs)):\n",
    "    pbar.set_description(lp)\n",
    "\n",
    "    dset_path = f'{DATA_DIR}/{lp}_dset.pt'\n",
    "    if os.path.exists(dset_path):\n",
    "        lp_dset = torch.load(dset_path)\n",
    "    else:\n",
    "        lp_dset = train_dset.filter(lambda x: x['lp'] == lp).shuffle(seed=SEED)\n",
    "        src_embs_dset = Dataset.from_dict({'src_emb': st.encode_multi_process(lp_dset['src'], pool, batch_size=128)})\n",
    "        lp_dset = datasets.concatenate_datasets([lp_dset, src_embs_dset], axis=1)\n",
    "        torch.save(lp_dset, dset_path)\n",
    "    lp2train_dset[lp] = lp_dset\n",
    "\n",
    "    ref_index_path = f'{DATA_DIR}/{lp}_ref_faiss.idx'\n",
    "    mt_index_path = f'{DATA_DIR}/{lp}_mt_faiss.idx'\n",
    "\n",
    "    lp2ref_index[lp] = faiss.read_index(ref_index_path) if os.path.exists(ref_index_path) else \\\n",
    "            prepare_faiss(st, pool, lp_dset['ref'], save_filepath=ref_index_path)\n",
    "    lp2mt_index[lp] = faiss.read_index(mt_index_path) if os.path.exists(mt_index_path) else \\\n",
    "            prepare_faiss(st, pool, lp_dset['mt'], save_filepath=mt_index_path)\n",
    "\n",
    "st.stop_multi_process_pool(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b0d10-3c2d-401b-aab5-e52e27444718",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591d577-8165-4831-b191-b51f71c646b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, lang=None):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    text = text.lower().strip()\n",
    "    if lang is not None:\n",
    "        text = f'{lang}: {text}'\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def tokenize_(data, tokenizer, max_length=MAX_LENGTH, add_lang=False):\n",
    "    src_lang, tgt_lang = map(lambda x: id2lang[x], data['lp'].split('-'))\n",
    "    output = {}\n",
    "    for field in ['src', 'ref', 'mt']:\n",
    "        result = tokenizer(preprocess(data[field], src_lang if field == 'src' else tgt_lang),\n",
    "                           truncation=True, max_length=max_length, padding=False)\n",
    "        if max_length is not None and result['input_ids'][-1] != tokenizer.eos_token_id \\\n",
    "            and len(result['input_ids']) < max_length:\n",
    "            result['input_ids'].append(tokenizer.eos_token_id)\n",
    "            result['attention_mask'].append(1)\n",
    "        output[f'{field}_input_ids'] = result['input_ids']\n",
    "        output[f'{field}_attention_mask'] = result['attention_mask']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e4584-759e-48ab-b899-50fd899f847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp2train_dset = DatasetDict(lp2train_dset).map(partial(tokenize_, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98b821-9612-43ad-b5c0-17dc7ceddcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset = test_dset.map(partial(tokenize_, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f11444-a76b-49eb-a5a3-3ad59f682844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWMTCL(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input,\n",
    "        inference=False,\n",
    "        train_batch_size=32\n",
    "    ):\n",
    "        self.inference = inference\n",
    "        assert isinstance(input, list) and (not inference or len(input) == 1)\n",
    "        if inference:\n",
    "            self.datasets = input\n",
    "            self.total_len = len(input[0])\n",
    "            return\n",
    "\n",
    "        self.total_len = 0\n",
    "        self.cumsum_lens = []\n",
    "        self.datasets = []\n",
    "        self.ref_indexes = []\n",
    "        self.mt_indexes = []\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.n_neighbors = (train_batch_size - 3) // 2\n",
    "\n",
    "        for part in input:\n",
    "            dataset, ref_index, mt_index = part\n",
    "            self.ref_indexes.append(ref_index)\n",
    "            self.mt_indexes.append(mt_index)\n",
    "            self.datasets.append(dataset)\n",
    "            self.total_len += len(dataset)\n",
    "            self.cumsum_lens.append(self.total_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def determine_data_index(self, idx):\n",
    "        prev_cumsum_len = 0\n",
    "        for i, cumsum_len in enumerate(self.cumsum_lens):\n",
    "            if idx < cumsum_len:\n",
    "                return i, idx - prev_cumsum_len\n",
    "            prev_cumsum_len = cumsum_len\n",
    "        raise ValueError(f'Index {idx} is not in valid range')\n",
    "\n",
    "    def __getitem_for_train(self, idx):\n",
    "        assert len(idx) == 1\n",
    "        idx = idx[0]\n",
    "\n",
    "        output = {}\n",
    "        i, idx = self.determine_data_index(idx)\n",
    "        dataset = self.datasets[i]\n",
    "        point = dataset[idx]\n",
    "        output = {'score_type': [point['score_type']]*self.train_batch_size, 'score': [point['score']]*self.train_batch_size}\n",
    "        for key in ['input_ids', 'attention_mask']:\n",
    "            output[key] = [point[f'{field}_{key}'] for field in ['src', 'ref', 'mt']]\n",
    "\n",
    "        src_emb = np.asarray(point['src_emb'])[None, :]\n",
    "        ref_index = self.ref_indexes[i]\n",
    "        mt_index = self.mt_indexes[i]\n",
    "\n",
    "        _, ref_I = ref_index.search(src_emb, k=self.n_neighbors+1+1)\n",
    "        _, mt_I = mt_index.search(src_emb, k=self.n_neighbors+1)\n",
    "\n",
    "        ref_I = ref_I.ravel()\n",
    "        mt_I = mt_I.ravel()\n",
    "\n",
    "        for j in range(self.n_neighbors+1):\n",
    "            if ref_I[j] == idx:\n",
    "                continue\n",
    "            far_point = dataset[int(ref_I[j])]\n",
    "            for key in ['input_ids', 'attention_mask']:\n",
    "                output[key].append(far_point[f'ref_{key}'])\n",
    "        for j in range(self.n_neighbors):\n",
    "            if mt_I[j] == idx:\n",
    "                continue\n",
    "            far_point = dataset[int(mt_I[j])]\n",
    "            for key in ['input_ids', 'attention_mask']:\n",
    "                output[key].append(far_point[f'mt_{key}'])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __getiten_for_test(self, idx):\n",
    "        output = {}\n",
    "        dataset = self.datasets[0]\n",
    "        points = dataset[idx]\n",
    "        for key in ['input_ids', 'attention_mask']:\n",
    "            output[key] = []\n",
    "            for field in ['src', 'ref']:\n",
    "                output[key] += points[f'{field}_{key}'] \n",
    "        return output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__getitem_for_train(idx) if not self.inference else self.__getiten_for_test(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1c94c-cd4e-44ef-855d-aa507e503b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorWithPaddingAndScore:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        padding=True,\n",
    "        max_length=None,\n",
    "        pad_to_multiple_of=None,\n",
    "        return_tensors=\"pt\"\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "        self.return_tensors = return_tensors \n",
    "\n",
    "    def __call__(self, features):\n",
    "        scores = []\n",
    "        score_types = []\n",
    "        for feature in features:\n",
    "            scores.append(feature.pop('score'))\n",
    "            score_types.append(feature.pop('score_type'))\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch['score'] = scores\n",
    "        batch['score_type'] = score_types\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ecbde5-87b8-443c-8b21-95faf5b880ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 4\n",
    "wmtcl_train_dset = DatasetWMTCL(\n",
    "    [(lp2train_dset[lp], lp2ref_index[lp], lp2mt_index[lp]) for lp in lang_pairs],\n",
    "    train_batch_size=train_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0970bba-e493-4ccf-8733-8dfb23d7fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmtcl_test_dset = DatasetWMTCL(\n",
    "    [test_dset],\n",
    "    inference=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5f6a9-7906-490d-a141-52926a49931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collator = DataCollatorWithPaddingAndScore(tokenizer, pad_to_multiple_of=4, max_length=MAX_LENGTH) \n",
    "test_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=4, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c3185-2597-4253-95ad-a10033c4b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(wmtcl_train_dset, batch_size=1, shuffle=True, pin_memory=True, collate_fn=train_collator)\n",
    "test_dataloader = DataLoader(wmtcl_test_dset, batch_size=16, shuffle=False, pin_memory=True, collate_fn=test_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc078657-c1f6-4f93-a93c-29c6718e0c6b",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e74453-4f23-49d6-beb7-0d38632b16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLossWMT(nn.Module):\n",
    "    \"\"\"\n",
    "    Full credit to https://zablo.net/blog/post/understanding-implementing-simclr-guide-eli5-pytorch/.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        negative_n, # n_neighbors from dataset initialization\n",
    "        score_type_weights=None, # prioritization of score types\n",
    "        temperature=0.05,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.negative_n = negative_n\n",
    "        self.score_type_weights = score_type_weights\n",
    "        self.register_buffer(\"temperature\", torch.tensor(temperature, device=device))\n",
    "            \n",
    "    def forward(self, embs: torch.Tensor, score: float, score_type: str):\n",
    "        embs = torch.nn.functional.normalize(embs, dim=1)\n",
    "        src_emb = embs[0:1]\n",
    "        tgt_embs = embs[1:]\n",
    "        similarity_vector = src_emb @ tgt_embs.T\n",
    "        similarity_vector = similarity_vector.squeeze()\n",
    "\n",
    "        ref_nom  = torch.exp(similarity_vector[0] / self.temperature)\n",
    "        ref_denom = ref_nom + torch.exp(similarity_vector[2 : 2+self.negative_n+1] / self.temperature).sum()\n",
    "        mt_nom  = self.score_type_weights[score_type] * score * torch.exp(similarity_vector[0] / self.temperature)\n",
    "        mt_denom = mt_nom + torch.exp(similarity_vector[2+self.negative_n+1 : 2+2*self.negative_n+1] / self.temperature).sum()\n",
    "        loss = -torch.log(ref_nom / ref_denom) - torch.log(mt_nom / mt_denom)\n",
    "        return loss\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb4bc4-b194-4b4d-b155-39423a671c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f27777-6d81-47f4-ba36-fab3c0b10636",
   "metadata": {},
   "source": [
    "```python\n",
    "max_memory = get_balanced_memory(\n",
    "     model,\n",
    "    max_memory=None,\n",
    "    no_split_module_classes=['MT5LayerSelfAttention', 'MT5LayerFF'],\n",
    "    dtype='float16',\n",
    "    low_zero=True,\n",
    ")\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model,\n",
    "    max_memory=max_memory,\n",
    "    no_split_module_classes=['MT5LayerSelfAttention', 'MT5LayerFF'],\n",
    "    dtype='float16'\n",
    ")\n",
    "\n",
    "model = dispatch_model(model, device_map=device_map)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae604c-c202-4eec-95a3-584c48f3911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232e899-b2af-4b54-8608-06b0a874803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "n_steps = n_epochs * len(wmtcl_train_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1f04d-6627-46ff-aace-b411213c9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters()\n",
    ")\n",
    "scheduler = get_scheduler('linear', optimizer, num_warmup_steps=1000, num_training_steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a096e-f3e4-46a2-bff8-6d8433fa32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ContrastiveLossWMT(wmtcl_train_dset.n_neighbors,\n",
    "                          score_type_weights={score_type2id['da']: 1, score_type2id['mqm']: 2, score_type2id['sqm']: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f380d-897a-46f0-a8d7-7c3136c11c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(gradient_accumulation_steps=4)\n",
    "train_dataloader, test_dataloader, model, optimizer, scheduler = accelerator.prepare(\n",
    "    train_dataloader, test_dataloader, model, optimizer, scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f68c1-d7e7-40c7-a9ff-042407405234",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm_notebook(range(n_epochs)):\n",
    "    model.train()\n",
    "    for batch in (pbar := tqdm_notebook(train_dataloader)):\n",
    "        with accelerator.accumulate(model):\n",
    "            score = batch.pop('score')[0]\n",
    "            score_type = batch.pop('score_type')[0]\n",
    "    \n",
    "            outputs = model(**batch)\n",
    "            outputs = mean_pooling(outputs.last_hidden_state, batch['attention_mask'])\n",
    "    \n",
    "            loss_ = loss(outputs, score, score_type)\n",
    "            free_()\n",
    "    \n",
    "            loss_.backward()\n",
    "            free_()\n",
    "    \n",
    "            optimizer.step()\n",
    "            free_()\n",
    "    \n",
    "            scheduler.step()\n",
    "            free_()\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            free_()\n",
    "\n",
    "            pbar.set_description(f'loss: {loss_.item()}')\n",
    "\n",
    "    model.eval()\n",
    "    total_correlation = 0.0\n",
    "    total_ = 0.0\n",
    "    if accelerator.is_local_main_process:\n",
    "        for batch in (pbar := tqdm_notebook(test_dataloader)):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "                outputs = mean_pooling(outputs.last_hidden_state, batch['attention_mask'])\n",
    "                outputs = torch.nn.functional.normalize(outputs, dim=1)\n",
    "                embs_src = outputs[0::2]\n",
    "                embs_ref = outputs[1::2]\n",
    "\n",
    "                correlation_ = (embs_src @ embs_ref.T).diag().sum().item()\n",
    "                total_correlation += correlation_\n",
    "                total_ += (len(batch) / 2)\n",
    "\n",
    "                pbar.set_description(f'src & ref correlation: {total_correlation / total_}')\n",
    "\n",
    "        print(f'src & ref correlation: {total_correlation / total_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80141c45-4a5b-47ce-98b1-84cf6d636c49",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
